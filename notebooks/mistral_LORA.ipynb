{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84056826-c8f9-4853-91c2-39559b483a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a40ecbc2834fc69c48e78538016ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4a8a45-ff5e-48d2-a188-8090bf2bf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c061ebbe-f1f5-42b4-b418-bb329f635dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../keys.json\", \"r\") as file:\n",
    "    token_data = json.load(file)\n",
    "HUGGINGFACE_TOKEN = token_data[\"huggingface_access_token\"]\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HUGGINGFACE_TOKEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68ea490-1046-481b-b4d5-db52bddba122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34746b23b2c14517afc3908b410cc2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510992f0a84f4314a55f74ed5395d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b8c5331dc044aa9a6916feb2aba1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17fcf498fbe48169b6cfb3cb5f1fccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484e168722094482a93755fed21eb211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ee76e3989049b1b119bd2718b35085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969155f0b25241bbb7d9f5d89e5a4ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15453e8fc310454589972b6a6c522949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the instruct-tuned model checkpoint\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "scratch_dir = \"../models\"\n",
    "\n",
    "# Load the tokenizer (force using the slow tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=scratch_dir)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=scratch_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d703557-233e-4520-8339-3918f3654ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed32c41e5074b739871610acec9a6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the LoRA configuration.\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # rank of update matrices\n",
    "    lora_alpha=32,            # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which modules to adapt (example modules)\n",
    "    lora_dropout=0.1,         # dropout probability for lora layers\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load and preprocess a dataset (here we use Wikitext-2 as an example)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952aa73d-db96-4db5-9020-22d560a0ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c44261-722c-45b2-953e-2a11ce80e637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944b00d89866430d95dc667d975d7253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the pad_token if it isn't already defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def tokenize_function(example):\n",
    "    outputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # adjust max_length as required\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = sample_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5641905c-7fcf-47a4-936f-c93d49aa4391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mistral\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # simulate larger batch sizes\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,                     # mixed precision training\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c696d82f-338a-4ae1-8063-044cd23d2f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718d21a7571a45dbb76ee67088864a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter weights (or the entire model if desired)\n",
    "model.save_pretrained(\"./lora_mistral_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958e8e7-fd84-4112-b659-433fb3691f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f2ed260-a311-4c8e-b132-47c57cc305ab",
   "metadata": {},
   "source": [
    "## Predict pipeline for Mistral-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77342171-4cc7-4cd4-b735-76e4fbc40ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca33bb3ebe745838af5e1420caa0e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the instruct-tuned model checkpoint\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "scratch_dir = \"../models\"\n",
    "\n",
    "# Load the tokenizer (force using the slow tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=scratch_dir)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=scratch_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7ba2b3-c3d2-4626-ba4b-66560fcdf0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a man named David. He was a shepherd in the land of Israel. He was tall, strong and handsome. But what really set him apart was his heart for God.\n",
      "\n",
      "David's family was part of the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example prompt for inference\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate output (adjust parameters like max_new_tokens as desired)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,   # maximum number of new tokens to generate\n",
    "        do_sample=True,      # use sampling; set to False for greedy decoding\n",
    "        temperature=0.7      # adjust temperature for randomness\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671feb9-dd6c-4872-b017-a0de6b31c14a",
   "metadata": {},
   "source": [
    "## Prediction pipeline (LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b693be3c-ab7f-436d-a3e6-6e6fdbcc68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5125ee3-f138-46e7-860c-467a92b5517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97956d730de4b88afcf64181993f4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the instruct-tuned model checkpoint\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "scratch_dir = \"../models\"\n",
    "\n",
    "# Load the tokenizer (force using the slow tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=scratch_dir)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=scratch_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871790bd-d0a0-4399-94bd-22aa1efc49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, a friend gave me a book with a simple title, The Bible. I had heard of it before, but I had never read it. I had no idea what I was in for. I never imagined it would become an essential part of my\n"
     ]
    }
   ],
   "source": [
    "# Load the LoRA adapter weights into the base model\n",
    "model = PeftModel.from_pretrained(model, \"./lora_mistral_adapter\")\n",
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "# Example prompt for inference\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate output (adjust parameters like max_new_tokens as desired)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,   # maximum number of new tokens to generate\n",
    "        do_sample=True,      # use sampling; set to False for greedy decoding\n",
    "        temperature=0.7      # adjust temperature for randomness\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabdcd5-d78b-4318-897a-1cb9401c0996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77d101-1bf6-406c-9e3b-795eb609c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
