{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c47e256-6b1f-4c1c-a4dd-bc604e78c755",
   "metadata": {},
   "source": [
    "## BLIP image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda03f40-d3f0-4e3d-9222-72771c3252ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d614d-a9ad-4e69-a09f-9fade8bfd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_image(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Downloads an image from a URL and returns a PIL Image.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    # Option 1: Use a local image by providing a path\n",
    "    image = Image.open(\"../data/tqa_train_val_test/train/teaching_images/biomes_6557.png\").convert(\"RGB\")\n",
    "    \n",
    "    # Option 2: Download an image from a URL (example image URL)\n",
    "    # image_url = \"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\"\n",
    "    # image = download_image(image_url)\n",
    "    \n",
    "    # Initialize BLIP processor and model with a custom cache directory\n",
    "    cache_directory = \"../models\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=cache_directory)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=cache_directory)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    inputs = processor(image, return_tensors=\"pt\", max_length=200, )\n",
    "    \n",
    "    # Generate caption (you can adjust parameters like max_length or num_beams if needed)\n",
    "    output_ids = model.generate(**inputs)\n",
    "    \n",
    "    # Decode the generated ids to text\n",
    "    caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"Generated Caption:\")\n",
    "    print(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a1d827-dea7-475b-925f-594e26695e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "a pyramid with the names of different types of plants and animals\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cc2db-4f30-4b6b-b0a1-b67ed4be67de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5347f7-a1e8-4b9f-9948-31ca83c0ef92",
   "metadata": {},
   "source": [
    "## LLAVA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00840b4-fadf-40ec-bbd4-3c6aff6a8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Optionally restrict to one GPU if needed\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193fda9-250f-4919-b92c-fbf0c0ef1b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089522cd112c4b2ea17335d66a55612d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load a local image\n",
    "image = Image.open(\"../data/tqa_train_val_test/train/teaching_images/acid_rain_formation_6507.png\").convert(\"RGB\")\n",
    "\n",
    "# Define your model name or local checkpoint path\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "cache_directory = \"../models\"\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "\n",
    "\n",
    "if not hasattr(processor, \"patch_size\"):\n",
    "    processor.patch_size = 14\n",
    "if not hasattr(processor, \"vision_feature_select_strategy\"):\n",
    "    processor.vision_feature_select_strategy = \"default\"  # Options: \"default\", \"first\", \"max\"\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"sequential\",\n",
    "    cache_dir=cache_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554e5cc8-fe0e-464c-8b92-d6b3617307d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA Response:\n",
      "### Human:  \n",
      "Please describe this diagram in detail.\n",
      "### Assistant:\n",
      "\n",
      "In the image, there is a diagram illustrating the process of acid rain formation. Acid rain is formed when pollutants, such as sulfur dioxide and nitrogen oxides, are released into the atmosphere. These pollutants react with moisture in the air to form sulfuric and nitric acids, which then fall to the ground as acid rain.\n",
      "\n",
      "The diagram shows the various stages of acid rain formation, including the release of pollutants into the atmosphere, the formation of sulfuric and nitric acids in the presence of moisture, and the precipitation of these acids as acid rain. Additionally, the diagram highlights the impact of acid rain on the environment, such as the corrosion of buildings, statues, and other structures, as well as the harm it can cause to plants and aquatic life.\n",
      "\n",
      "Overall, the image provides a comprehensive overview of the process of acid rain formation and its consequences on the environment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use structured prompt format\n",
    "prompt = \"### Human: <image>\\nPlease describe this diagram in detail.\\n### Assistant:\"\n",
    "\n",
    "# Process image and text\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure inputs are on the correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate response\n",
    "output_ids = model.generate(**inputs, max_new_tokens=500, num_beams=5)\n",
    "\n",
    "# Decode response\n",
    "response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"LLaVA Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40be3a-b3f1-4ad0-b7cf-5ad960979d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1bf4a0-b952-4340-8003-c544a74116ad",
   "metadata": {},
   "source": [
    "## Image captioning folder wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa119571-73d4-4d60-b05c-f30966b4d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optionally restrict to a single GPU. Adjust the device number as needed.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# Define paths and model/checkpoint names.\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "cache_directory = \"../models\"\n",
    "input_folder = \"../data/tqa_train_val_test/test/teaching_images\"  # Folder with images\n",
    "output_folder = \"../data/tqa_train_val_test/test/teaching_images_llava_captions\"  # Folder to save txt outputs\n",
    "\n",
    "# Create the output folder if it doesn't exist.\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6415684-1a81-40c7-ba23-cffdfc675e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334b543a974f43c7aadf0ab96d877ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load processor and set required attributes to avoid deprecation warnings.\n",
    "processor = AutoProcessor.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "if not hasattr(processor, \"patch_size\"):\n",
    "    processor.patch_size = 14\n",
    "if not hasattr(processor, \"vision_feature_select_strategy\"):\n",
    "    processor.vision_feature_select_strategy = \"default\"  # Options: \"default\", \"first\", \"max\"\n",
    "\n",
    "# Force the model to load sequentially on one GPU.\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"sequential\",\n",
    "    cache_dir=cache_directory\n",
    ")\n",
    "\n",
    "# Use a structured prompt format.\n",
    "prompt = \"### Human: <image>\\nPlease describe this diagram in detail.\\n### Assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcad90ca-c300-4828-a2b4-b1d3a263071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Given an image path, load the image, generate a response with LLaVA,\n",
    "    and return the cleaned description (with prompt tokens and instructions removed).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {image_path}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response.\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=256, num_beams=5)\n",
    "    response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Clean up the response by removing the structured prompt parts.\n",
    "    lines = response.splitlines()\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"###\"):\n",
    "            continue\n",
    "        if \"Please describe this diagram in detail.\" in line:\n",
    "            line = line.replace(\"Please describe this diagram in detail.\", \"\")\n",
    "        if line.strip():\n",
    "            clean_lines.append(line.strip())\n",
    "    clean_response = \"\\n\".join(clean_lines).strip()\n",
    "    \n",
    "    return clean_response\n",
    "\n",
    "def process_folder(input_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process all image files in input_dir, generate a description for each,\n",
    "    and save the output in output_dir as a text file (one per image),\n",
    "    while displaying a progress bar.\n",
    "    \"\"\"\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\n",
    "    files = [f for f in os.listdir(input_dir) if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    \n",
    "    print(f\"Found {len(files)} images in {input_dir}.\")\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Processing images\", dynamic_ncols=True):\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        description = generate_description(image_path)\n",
    "        \n",
    "        output_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebb959c-2fba-4655-9b09-8ca186591f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 images in ../data/tqa_train_val_test/test/teaching_images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n",
      "Processing images: 100%|██████████| 31/31 [05:38<00:00, 10.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the processing over the entire folder.\n",
    "process_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bd9ef-f5e9-456c-bdd0-39f9a89657dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
