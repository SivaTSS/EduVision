{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda03f40-d3f0-4e3d-9222-72771c3252ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827d614d-a9ad-4e69-a09f-9fade8bfd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this code, first install the dependencies:\n",
    "# pip install transformers torch torchvision pillow\n",
    "\n",
    "\n",
    "\n",
    "def download_image(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Downloads an image from a URL and returns a PIL Image.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    # Option 1: Use a local image by providing a path\n",
    "    image = Image.open(\"../data/tqa_train_val_test/train/teaching_images/biomes_6557.png\").convert(\"RGB\")\n",
    "    \n",
    "    # Option 2: Download an image from a URL (example image URL)\n",
    "    # image_url = \"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\"\n",
    "    # image = download_image(image_url)\n",
    "    \n",
    "    # Initialize BLIP processor and model with a custom cache directory\n",
    "    cache_directory = \"../models\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=cache_directory)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=cache_directory)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    inputs = processor(image, return_tensors=\"pt\", max_length=200, )\n",
    "    \n",
    "    # Generate caption (you can adjust parameters like max_length or num_beams if needed)\n",
    "    output_ids = model.generate(**inputs)\n",
    "    \n",
    "    # Decode the generated ids to text\n",
    "    caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"Generated Caption:\")\n",
    "    print(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a1d827-dea7-475b-925f-594e26695e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "a pyramid with the names of different types of plants and animals\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cc2db-4f30-4b6b-b0a1-b67ed4be67de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5347f7-a1e8-4b9f-9948-31ca83c0ef92",
   "metadata": {},
   "source": [
    "## LLAVA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00840b4-fadf-40ec-bbd4-3c6aff6a8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Optionally restrict to one GPU if needed:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3193fda9-250f-4919-b92c-fbf0c0ef1b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089522cd112c4b2ea17335d66a55612d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load a local image\n",
    "image = Image.open(\"../data/tqa_train_val_test/train/teaching_images/acid_rain_formation_6507.png\").convert(\"RGB\")\n",
    "\n",
    "# Define your model name or local checkpoint path\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "cache_directory = \"../models\"\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "\n",
    "# âœ… Set required processing attributes to avoid deprecation errors\n",
    "if not hasattr(processor, \"patch_size\"):\n",
    "    processor.patch_size = 14  # Adjust based on the model version\n",
    "if not hasattr(processor, \"vision_feature_select_strategy\"):\n",
    "    processor.vision_feature_select_strategy = \"default\"  # Options: \"default\", \"first\", \"max\"\n",
    "\n",
    "# Force model to load sequentially on one GPU\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"sequential\",\n",
    "    cache_dir=cache_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554e5cc8-fe0e-464c-8b92-d6b3617307d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA Response:\n",
      "### Human:  \n",
      "Please describe this diagram in detail.\n",
      "### Assistant:\n",
      "\n",
      "In the image, there is a diagram illustrating the process of acid rain formation. Acid rain is formed when pollutants, such as sulfur dioxide and nitrogen oxides, are released into the atmosphere. These pollutants react with moisture in the air to form sulfuric and nitric acids, which then fall to the ground as acid rain.\n",
      "\n",
      "The diagram shows the various stages of acid rain formation, including the release of pollutants into the atmosphere, the formation of sulfuric and nitric acids in the presence of moisture, and the precipitation of these acids as acid rain. Additionally, the diagram highlights the impact of acid rain on the environment, such as the corrosion of buildings, statues, and other structures, as well as the harm it can cause to plants and aquatic life.\n",
      "\n",
      "Overall, the image provides a comprehensive overview of the process of acid rain formation and its consequences on the environment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use structured prompt format\n",
    "prompt = \"### Human: <image>\\nPlease describe this diagram in detail.\\n### Assistant:\"\n",
    "\n",
    "# Process image and text\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure inputs are on the correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate response\n",
    "output_ids = model.generate(**inputs, max_new_tokens=500, num_beams=5)\n",
    "\n",
    "# Decode response\n",
    "response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"LLaVA Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40be3a-b3f1-4ad0-b7cf-5ad960979d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
