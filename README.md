# ğŸ“š EduVision: AI-Powered Visual Textbook Companion

**EduVision** is an intelligent learning assistant that transforms raw educational text into **interactive mindmaps** and **animated videos** to support visual learning. It leverages LLMs (like GPT-4o), Manim animations, Mermaid diagrams, and TTS to make textbook concepts engaging and accessible.

---

## âœ¨ Features

### **Mindmap Generation**
  - Extracts key concepts from text
  - Generates clean [Mermaid.js](https://mermaid-js.github.io/) mindmaps
  - Includes summaries and SVG rendering

### **Animated Video Pipeline**
  - Converts topics into step-wise Manim animations
  - Uses LLMs to write and debug Manim code
  - Auto-generates narration and syncs it with video
  - Supports final video concatenation

<div style="text-align: center;">
  <img src="assets/animation_pipeline.png" alt="Animation pipeline" style="max-height: 500px;">
</div>

### **Web UI with Gradio**
  - Text input â†’ mindmap + summary
  - Text input â†’ animated explainer video
  - Easily launchable interactive interface

<div style="text-align: center;">
  <img src="assets/gradio_ui.png" alt="Gradio UI" style="max-height: 500px;">
</div>

---

## Mindmap Training Pipeline

EduVisionâ€™s mindmap generator is backed by a supervisedâ€¯+â€¯RLHF training workflow:

### **Dataset (TQA)**  
   - **666 training samples** and **200 test samples** from the Textbook Question Answering corpus.

<div style="text-align: center;">
  <img src="assets/dataset.png" alt="Dataset" style="max-height: 500px;">
</div>

### **Preprocessing**  
   - **Image Captioning**: Use LLAVA to convert textbook figures into detailed captions.  
   - **Multimodal Fusion**: Combine captions with the original text for richer context.

<div style="text-align: center;">
  <img src="assets/data_preparation.png" alt="Data Preprocessing" style="max-height: 300px;">
</div>

### **Supervised Fineâ€‘Tuning**  
   - Base model: **Mistral-7B-Instruct-v0.3** (4â€‘bit quantized).  
   - Adapter: **qLoRA** applied to learn Mermaidâ€‘UML mindmap structure.  
   - Input: Textâ€¯+â€¯captions â†’ Output: Mermaid UML list with exactly one root node.

### **Reinforcement Learning with Human Feedback (RLHF)**  

<div style="text-align: center;">
  <img src="assets/rl_pipeline.png" alt="RLHF Pipeline" style="max-height:400px;">
</div>

   - **PPO** applied to the qLoRAâ€‘tuned model.  
   - **Simulated feedback**: GPTâ€‘4oâ€‘mini generates two candidates; selects the better as ground truth.  
   - **Reward Model**: GPTâ€‘4oâ€‘mini scores candidates on Cognitive Quality and Structural Integrity; normalized to [0,1].  
   - **Training loop**: At each step, the policy generates a mindmap, reward is computed, and PPO updates the LoRA adapter under a KL penalty against a frozen reference.

<div style="text-align: center;">
  <img src="assets/rlhf_training_pipeline.png" alt="RLHF Pipeline params" style="max-height:400px;">
</div>

### **Inference**  
   - Instructionâ€‘style prompt ensures minimal, valid Mermaid syntax with one central node.  
   - Entire pipelineâ€”from raw text to final Mermaid listâ€”is fully automated in `src/mindmap.py`.

<div style="text-align: center;">
  <img src="assets/mindmap_inference_pipeline.png" alt="mindmap Inference" style="max-height:400px;">
</div>

---
## ğŸ”§ Install Dependencies

Install Python packages:

```bash
pip install -r requirements.txt
```

### **Mindmap visualization setup instruction**

```
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash
source ~/.nvm/nvm.sh
nvm install 18
nvm use 18
node -v
npm install -g @mermaid-js/mermaid-cli
mmdc -V
```
### **Manim and animation setup instruction**

```
sudo apt update
sudo apt install build-essential
```

```
sudo apt install pkg-config cmake libcairo2-dev
sudo apt install libpango1.0-dev
sudo apt install ffmpeg
sudo apt install texlive texlive-latex-extra
```


## ğŸ§  Architecture Overview

```
ğŸ“¦ EduVision
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                # Gradio app (UI logic)
â”‚   â”œâ”€â”€ animation_gen.py      # Manim + narration + TTS pipeline
â”‚   â”œâ”€â”€ llm_loader.py         # Loads OpenAI keys and LLM
â”‚   â”œâ”€â”€ mindmap.py            # Mindmap generation via Mermaid
â”‚   â””â”€â”€ summary.py            # Text summarization
â”œâ”€â”€ data/                     # Training/eval data (selectively included)
â”œâ”€â”€ notebooks/                # Dev notebooks
â”œâ”€â”€ models/                   # Optional model exports
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Example Use Cases

- Teachers can animate chapters and create narrated explainers
- Students can visualize relationships between concepts
- EdTech platforms can integrate auto-summary and mindmapping

---

## ğŸ§­ Usage Instructions

### ğŸ”‘ API Key Setup

Create a `keys.json` file at the root directory:

```json
{
  "openai_api_key": "your_openai_api_key_here",
  "huggingface_access_token": "your_huggingface_access_token_here"
}
```

### â–¶ï¸ Run the App

From the project directory:

```bash
python src/app.py
```

Then open the Gradio interface in your browser and start exploring!

---

## ğŸ“‚ Output Structure

Generated media and intermediate outputs go to:

```
outputs/
â”œâ”€â”€ animation_outline.yaml
â”œâ”€â”€ audio/
â”œâ”€â”€ final/
â”œâ”€â”€ temp/
â””â”€â”€ videos/
```

---

## ğŸ§° Tools Used

- [LangChain](https://github.com/hwchase17/langchain) â€“ LLM interface
- [Manim](https://www.manim.community/) â€“ 2D math animation engine
- [gTTS](https://pypi.org/project/gTTS/) â€“ Google Text-to-Speech
- [Gradio](https://www.gradio.app/) â€“ Interactive web app
- [Mermaid.js](https://mermaid-js.github.io/) â€“ Diagram rendering

---



